{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:05:35.695840Z","iopub.status.busy":"2023-05-23T04:05:35.695230Z","iopub.status.idle":"2023-05-23T04:05:51.177437Z","shell.execute_reply":"2023-05-23T04:05:51.176204Z","shell.execute_reply.started":"2023-05-23T04:05:35.695785Z"},"papermill":{"duration":13.875729,"end_time":"2023-05-08T06:10:55.254188","exception":false,"start_time":"2023-05-08T06:10:41.378459","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from konlpy) (4.9.2)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.10/site-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install konlpy  # 토큰화에 사용할 konlpy 라이브러리 설치"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:09:03.171852Z","iopub.status.busy":"2023-05-23T04:09:03.170809Z","iopub.status.idle":"2023-05-23T04:09:03.178839Z","shell.execute_reply":"2023-05-23T04:09:03.177779Z","shell.execute_reply.started":"2023-05-23T04:09:03.171795Z"},"papermill":{"duration":0.022219,"end_time":"2023-05-08T06:10:55.287013","exception":false,"start_time":"2023-05-08T06:10:55.264794","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os, random\n","from tqdm import tqdm # 진행도 시각화를 위한 라이브러리\n","\n","seed=42\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","random.seed(seed)\n","np.random.seed(seed)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.009253,"end_time":"2023-05-08T06:10:55.306186","exception":false,"start_time":"2023-05-08T06:10:55.296933","status":"completed"},"tags":[]},"source":["# 데이터 불러오기"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:09:04.656492Z","iopub.status.busy":"2023-05-23T04:09:04.655752Z","iopub.status.idle":"2023-05-23T04:09:05.568124Z","shell.execute_reply":"2023-05-23T04:09:05.567042Z","shell.execute_reply.started":"2023-05-23T04:09:04.656457Z"},"papermill":{"duration":0.863577,"end_time":"2023-05-08T06:10:56.179319","exception":false,"start_time":"2023-05-08T06:10:55.315742","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(149993, 2)\n","(49999, 1)\n"]}],"source":["train_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_train.csv\", index_col=0)\n","test_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_test.csv\", index_col=0)\n","print(train_data.shape)\n","print(test_data.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:09:05.571096Z","iopub.status.busy":"2023-05-23T04:09:05.569886Z","iopub.status.idle":"2023-05-23T04:09:05.587651Z","shell.execute_reply":"2023-05-23T04:09:05.586485Z","shell.execute_reply.started":"2023-05-23T04:09:05.571050Z"},"papermill":{"duration":0.044922,"end_time":"2023-05-08T06:10:56.234099","exception":false,"start_time":"2023-05-08T06:10:56.189177","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>rating</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9324809</th>\n","      <td>배우들의 인생연기가 돋보였던... 최고의 드라마</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9305425</th>\n","      <td>아 혜리 보고싶다 ... 여군좀 ㅠ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5239110</th>\n","      <td>눈이 팅팅..... 정말 ,..... 대박이다......</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9148159</th>\n","      <td>캐슬린 터너의 보디는 볼만했다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6144938</th>\n","      <td>진짜 최고였다.</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                  review  rating\n","id                                              \n","9324809       배우들의 인생연기가 돋보였던... 최고의 드라마       1\n","9305425              아 혜리 보고싶다 ... 여군좀 ㅠ       0\n","5239110  눈이 팅팅..... 정말 ,..... 대박이다......       1\n","9148159                 캐슬린 터너의 보디는 볼만했다       0\n","6144938                         진짜 최고였다.       1"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train_data.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:09:05.922282Z","iopub.status.busy":"2023-05-23T04:09:05.921927Z","iopub.status.idle":"2023-05-23T04:09:05.927845Z","shell.execute_reply":"2023-05-23T04:09:05.926797Z","shell.execute_reply.started":"2023-05-23T04:09:05.922255Z"},"papermill":{"duration":0.032904,"end_time":"2023-05-08T06:10:56.282527","exception":false,"start_time":"2023-05-08T06:10:56.249623","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["x_train = train_data[\"review\"]\n","y_train = np.array(train_data[\"rating\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.01003,"end_time":"2023-05-08T06:10:56.302834","exception":false,"start_time":"2023-05-08T06:10:56.292804","status":"completed"},"tags":[]},"source":["# 자연어 전처리\n","## \\[Empty Module #1\\] 데이터 전처리\n","### 데이터 전처리 수행\n","> 먼저, 리뷰를 분류하는데 도움이 되거나, 머신러닝 처리에 어려운 단어들을 제거해봅시다.\n","1. 아래 조건에 맞는 정규표현식을 작성하여 영어와 한글 문자를 제외한 특수문자나 이모지, 숫자 등을 제거해봅시다.\n","  - <mark>한글 문자(초성 제외), 영어 대문자, 영어 소문자, 띄어쓰기 이외의 문자를 제외</mark>하는 정규표현식 작성\n","2. 영어 단어의 경우 같은 단어들이 같은 토큰으로 분류될 수 있도록 <mark>대문자로 통일</mark>해줍니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:16:30.573563Z","iopub.status.busy":"2023-05-23T04:16:30.573200Z","iopub.status.idle":"2023-05-23T04:16:30.580363Z","shell.execute_reply":"2023-05-23T04:16:30.579257Z","shell.execute_reply.started":"2023-05-23T04:16:30.573534Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["안녕HELLO반갑다\n"]}],"source":["import re\n","\n","def apply_regex(pattern, text):  # 정규표현식을 이용한 필터링 적용\n","    text = re.sub(pattern, \"\", text)  # 정규표현식 패턴에 맞는 값들을 텍스트에서 제거\n","    text = text.upper()# 영어들을 찾아 대문자로 치환하는 코드 작성\n","    return text\n","\n","text = \"안녕 Hello!!!:)ㅎㅎ반갑다.ㅠㅠ__\"\n","#pattern = '[^가-힣A-Za-z]'\n","pattern = '[^\\w\\s]|[ㄱ-ㅎ]|[ㅏ-ㅠ]|_'\n","output = apply_regex(pattern, text)\n","print(output)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:18:21.354730Z","iopub.status.busy":"2023-05-23T04:18:21.353760Z","iopub.status.idle":"2023-05-23T04:18:22.547530Z","shell.execute_reply":"2023-05-23T04:18:22.546381Z","shell.execute_reply.started":"2023-05-23T04:18:21.354695Z"},"papermill":{"duration":0.982216,"end_time":"2023-05-08T06:10:57.296264","exception":false,"start_time":"2023-05-08T06:10:56.314048","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_31/1444518553.py:18: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  x_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]\n","pre-processing data: 100%|██████████| 149993/149993 [00:01<00:00, 127821.79it/s]\n"]}],"source":["##########################################################################################\n","# Empty Module #1\n","# 입력: 자연어 상태의 리뷰 텍스트\n","# 출력: 한글(초성 제외), 영어 대문자, 띄어쓰기로만 구성된 텍스트 \n","# 입력 예시: \"안녕 Hello!!!:)ㅎㅎ반갑다.\"\n","# 출력 예시: \"안녕 HELLO반갑다\"\n","##########################################################################################\n","import re\n","\n","# 여기에 정규표현식 코드 작성\n","pattern = '[^\\w\\s]|[ㄱ-ㅎ]|[ㅏ-ㅣ]|_'\n","\n","def apply_regex(pattern, text):  # 정규표현식을 이용한 필터링 적용\n","    text = re.sub(pattern, \"\", text)  # 정규표현식 패턴에 맞는 값들을 텍스트에서 제거\n","    text = text.upper() # 영어들을 찾아 대문자로 치환하는 코드 작성\n","    return text\n","\n","x_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.014265,"end_time":"2023-05-08T06:10:57.323679","exception":false,"start_time":"2023-05-08T06:10:57.309414","status":"completed"},"tags":[]},"source":["## \\[Empty Module #2\\] 단어 토큰화\n","### Open Korean Text(OKT)를 이용한 단어 토큰화(Tokenization)\n","- 한국어 자연어 처리 라이브러리인 konlpy의 OKT 래퍼를 통하여 문장을 단어로 토큰화해봅시다.\n","- 아래 도큐먼트를 참고하여 토큰화를 수행합니다.\n","  - <mark>이때, OKT 클래스의 특정 매개변수를 이용해 어근화를 진행해줍니다. (도큐먼트 참고)</mark>\n","- konlpy 도큐먼트: https://konlpy.org/ko/latest/api/konlpy.tag/#okt-class\n","- OKT 도큐먼트: https://github.com/open-korean-text/open-korean-text"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:21:25.145503Z","iopub.status.busy":"2023-05-23T04:21:25.145113Z","iopub.status.idle":"2023-05-23T04:21:26.625049Z","shell.execute_reply":"2023-05-23T04:21:26.623993Z","shell.execute_reply.started":"2023-05-23T04:21:25.145472Z"},"papermill":{"duration":1.373034,"end_time":"2023-05-08T06:10:58.70747","exception":false,"start_time":"2023-05-08T06:10:57.334436","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["##########################################################################################\n","# Empty Module #2\n","# 입력: 자연어 상태의 리뷰 데이터\n","# 출력: 토큰화와 과정을 거쳐 단어들의 리스트로 변환된 데이터\n","# 입력 예시: \"커피는 역시 학생회관 커피\"\n","# 출력 예시: [\"커피\", \"는\", \"역시\", \"학생\", \"회관\", \"커피\"]\n","##########################################################################################\n","from konlpy.tag import Okt\n","okt = Okt()\n","\n","def tokenize_words(sentence):\n","    # 문장을 형태소 단위로 토큰화하는 코드 작성\n","    sentence_tokenized = okt.morphs(sentence) # 여기에 코드 작성\n","    sentence_tokenized = okt. \n","    return sentence_tokenized"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T04:21:30.081792Z","iopub.status.busy":"2023-05-23T04:21:30.081401Z"},"papermill":{"duration":573.138395,"end_time":"2023-05-08T06:20:31.856663","exception":false,"start_time":"2023-05-08T06:10:58.718268","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["tokenizing data:   2%|▏         | 3495/149993 [00:26<07:53, 309.42it/s]"]}],"source":["# 약 10-15분 정도 소요됩니다. \n","x_train_tokenized = [tokenize_words(x) for x in tqdm(x_train_preprocessed, desc=\"tokenizing data\")]"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.226231,"end_time":"2023-05-08T06:20:32.313486","exception":false,"start_time":"2023-05-08T06:20:32.087255","status":"completed"},"tags":[]},"source":["## \\[Empty Module #3\\] 불용어 제거\n","- 조사를 비롯한 불용어들은 많은 횟수 등장하지만, 리뷰의 긍정과 부정 여부를 판단하는데는 도움이 되지 않습니다.\n","- 데이터에서 아래 리스트로 정의된 불용어들을 제거해줍니다."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.425199,"end_time":"2023-05-08T06:20:33.964564","exception":false,"start_time":"2023-05-08T06:20:32.539365","status":"completed"},"tags":[]},"outputs":[],"source":["stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']  #별다른 의미가 없는 불용어들\n","\n","def exclude_stopwords(text):\n","    # 불용어들을 제거하는 코드 작성해줘\n","\n","    text = [word for word in text if not word in stopwords] # 위 리스트에 포함된 불용어들을 제거하는 코드 작성\n","    return \n","\n","x_train_stopwords_excluded = [exclude_stopwords(x) for x in x_train_tokenized]"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.227527,"end_time":"2023-05-08T06:20:34.416544","exception":false,"start_time":"2023-05-08T06:20:34.189017","status":"completed"},"tags":[]},"source":["## \\[Empty Module #4\\] 단어 임베딩\n","### 단어 임베딩 코드 구현\n","- 토큰화를 거쳐 분리된 단어들을 하나의 정수 값으로 매핑해주는 희소 표현법을 직접 구현해봅시다.\n","- 입력된 단어가 새로운 단어라면 새로운 정수 값을 할당하고, 이전에 등장한 단어라면 이전에 할당한 정수를 할당하는 함수를 작성합니다.\n","  - 단, <mark>테스트 데이터에 대해서는 새로운 단어가 등장하면 값을 할당하지 않습니다.</mark>"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.236545,"end_time":"2023-05-08T06:20:34.876645","exception":false,"start_time":"2023-05-08T06:20:34.6401","status":"completed"},"tags":[]},"outputs":[],"source":["##########################################################################################\n","# Empty Module #4\n","# 입력: 단어 토큰화된 데이터\n","# 출력: 임베딩 과정을 거쳐, 각 단어가 하나의 실수 값으로 표현된 데이터\n","# 입력 예시: [\"커피\", \"역시\", \"학생\", \"회관\", \"커피\"]\n","# 출력 예시: [0, 1, 2, 3, 0]\n","##########################################################################################\n","\n","embedding_dict = dict()  # 단어 임베딩을 위한 딕셔너리\n","embedding_value = 0\n","\n","def embed_tokens(sentence_tokenized, mode):\n","    assert mode.upper() in [\"TRAIN\", \"TEST\"] # assert문을 이용해 mode가 \"TRAIN\" 또는 \"TEST\"인지 확인\n","    global embedding_value\n","    \n","    sentence_embedded = list()\n","    for word in sentence_tokenized:\n","        # 코드 작성\n","        if word not in embedding_dict.keys():\n","            embedding_dict[word] = embedding_value\n","            embedding_value += 1\n","        sentence_embedded.append(embedding_dict[word]) \n","    return sentence_embedded"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.328669,"end_time":"2023-05-08T06:20:36.428533","exception":false,"start_time":"2023-05-08T06:20:35.099864","status":"completed"},"tags":[]},"outputs":[],"source":["# 실행 시간이 제법 소요됩니다. 비정상이 아니니 걱정하지 않으셔도 됩니다.\n","x_train_embedded = [embed_tokens(x, mode=\"TRAIN\") for x in tqdm(x_train_stopwords_excluded, desc=\"embedding data\")]\n","print(x_train_embedded[:5])\n","print(\"총 %d개의 단어가 임베딩되었습니다.\"%(embedding_value))"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.284019,"end_time":"2023-05-08T06:20:36.936606","exception":false,"start_time":"2023-05-08T06:20:36.652587","status":"completed"},"tags":[]},"source":["## \\[Empty Module #5\\] 문장 벡터화\n","### Bag of Words 방법을 사용한 문장 벡터화\n","- 캐글 프로젝트 설명 페이지의 Bag of Words 방법 설명을 참고하여 Bag of Words 방법을 직접 구현해봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":7.419859,"end_time":"2023-05-08T06:20:44.579436","exception":false,"start_time":"2023-05-08T06:20:37.159577","status":"completed"},"tags":[]},"outputs":[],"source":["##########################################################################################\n","# Empty Module #5\n","# 입력: 임베딩 과정을 거친 데이터\n","# 출력: BoW 형태로 변환되어, M차원의 고정된 크기를 가진 벡터로 변환된 데이터\n","# 힌트: np.zeros((2, 3))는 [2, 3] 크기의 0으로 가득 찬 행렬을 생성합니다.\n","##########################################################################################\n","\n","# 전체 단어의 수를 M이라고 할 때, BoW는 M차원의 벡터로 표현됩니다.\n","M = embedding_value\n","\n","\n","def to_BoW_representation(x):\n","    shape = \n","    x_BoW = np.zeros(shape)\n","    for i in tqdm(range(len(x)), desc=\"making BoW representation\"):\n","        # 여기에 BoW 구현 코드 작성\n","        for j in range(len(x[i])):\n","            x_BoW[i][x[i][j]] += 1\n","            \n","\n","    return x_BoW\n","x_train_BoW = to_BoW_representation(x_train_embedded)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.226259,"end_time":"2023-05-08T06:20:45.030446","exception":false,"start_time":"2023-05-08T06:20:44.804187","status":"completed"},"tags":[]},"source":["## \\[Empty Module #6\\] 차원 축소\n","- 우리가 만든 BoW는 수많은 리뷰에 등장하는 모든 단어들을 사용하여 만들어졌기 때문에, 엄청난 양의 단어들을 가지고 있습니다.\n","- 그러나, 실제로 영화 리뷰에 쓰이는 단어들은 이보다 적기 때문에, 많은 단어들이 전체 데이터에서 실제로는 한번도 등장하지 않거나, 매우 조금 등장하면서 공간을 차지하고 있을 것 입니다.\n","- 데이터의 크기를 줄여 머신러닝 모델이 중요한 정보에 집중할 수 있도록 해봅시다.\n","- <mark>학습 데이터에서 50번 미만으로 등장한 단어들을 제외</mark>해줍니다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##########################################################################################\n","# Empty Module #6\n","# 입력: BoW 형태로 변환된 (N, M) 크기의 데이터\n","# 출력: 등장 빈도가 적은 단어들을 제외한 (N, m) 크기의 더 작은 데이터\n","##########################################################################################\n","def exclude_rare_words(x, threshold):\n","    # 코드 작성\n","    x = x.sum(axis=0)\n","    b = np.where(x > threshold)\n","    "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["array([[2., 4., 5.],\n","       [1., 0., 0.],\n","       [0., 0., 0.]])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["\n","a  = np.array([[0, 1., 2., 3., 4., 5.],\n","                [1., 0., 1., 0., 0., 0.],\n","                [1., 0., 0., 0., 0., 0.]])\n","\n","def exclude_rare_words(x, threshold):\n","    x_sum = x.sum(axis=0)\n","    b = [x[i] for i in np.where(x_sum > threshold)[0]]\n","    return b\n","c = exclude_rare_words(a, 0)\n","\n","np.delete(a, [0, 1, 3], axis=1)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1. 1. 1. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0.]]\n","[1. 1. 2. 0. 0. 0.]\n","[array([1., 1., 1., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0.])]\n"]},{"data":{"text/plain":["[array([1., 1., 1., 0., 0., 0.]),\n"," array([0., 0., 1., 0., 0., 0.]),\n"," array([0., 0., 0., 0., 0., 0.])]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\n","a  = np.array([[1., 1., 1., 0., 0., 0.],\n","       [0., 0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0.]])\n","\n","def exclude_rare_words(x, threshold):\n","    x_sum = x.sum(axis=0)\n","    b = [x[i] for i in np.where(x_sum > threshold)[0]]\n","    x =\n","    print(b)\n","    return b\n","c = exclude_rare_words(a, 0)\n","c\n","# numpy array에서 axis = 1으로 지우는 방법\n","# np.delete(a, 1, axis=1)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[26 15 15]\n","(array([0, 1, 2]),)\n"]}],"source":["import numpy as np\n","a = np.array([[15, 8, 12], [11, 7, 3]])\n","b = a.sum(axis=0)\n","print(b) # [26 15 15]\n","c = np.where(b > 10) # (array([0, 0, 1]), array([0, 2, 0]))\n","print(c)"]},{"cell_type":"code","execution_count":21,"metadata":{"papermill":{"duration":0.236099,"end_time":"2023-05-08T06:20:45.493465","exception":false,"start_time":"2023-05-08T06:20:45.257366","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([[4., 5.],\n","       [1., 1.],\n","       [1., 1.]])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["##########################################################################################\n","# Empty Module #6\n","# 입력: BoW 형태로 변환된 (N, M) 크기의 데이터\n","# 출력: 등장 빈도가 적은 단어들을 제외한 (N, m) 크기의 더 작은 데이터\n","##########################################################################################\n","x_train_BoW  = np.array([[0., 1., 2., 3., 4., 5.],\n","       [1., 1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 1., 1.]])\n","\n","def exclude_rare_words(x, limit):\n","    # 코드 작성\n","    x_sum = x.sum(axis=0)\n","    indices = np.where(x_sum > limit)[0]\n","    return indices\n","\n","indices = exclude_rare_words(x_train_BoW, limit=50)\n","x_train_BoW_reduced = x_train_BoW[:, indices]\n","x_train_BoW_reduced"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":7.501002,"end_time":"2023-05-08T06:20:53.222732","exception":false,"start_time":"2023-05-08T06:20:45.72173","status":"completed"},"tags":[]},"outputs":[],"source":["# 힌트\n","# 1. 먼저 전체 데이터에서 각 단어가 등장한 횟수를 세어보세요.\n","# 2. 그 다음, 등장 횟수가 50회 미만인 단어들을 찾습니다.\n","# 3. 해당 단어들을 데이터에서 제거하는 코드를 작성합니다.\n","# 4. 설계를 잘 하고 구현을 시작해야 어렵지 않습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":12.114323,"end_time":"2023-05-08T06:21:05.627314","exception":false,"start_time":"2023-05-08T06:20:53.512991","status":"completed"},"tags":[]},"outputs":[],"source":["# 여기에 코드 작성\n","\n","print(\"원본 BoW 크기:\", x_train_BoW.shape)\n","print(\"차원 축소 후 크기:\", x_train_BoW_reduced.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.22558,"end_time":"2023-05-08T06:21:06.079455","exception":false,"start_time":"2023-05-08T06:21:05.853875","status":"completed"},"tags":[]},"source":["## \\[Empty Module #7\\]  분류 수행 및 제출: Bag of Words\n","- 이제 모든 문장이 고정된 크기 $m$ 차원의 벡터로 변환되었습니다.\n","- 원하는 모델을 사용하여, 각 문장의 영화에 대한 긍정적인 리뷰인지, 부정적인 리뷰인지 분류해봅시다.(Baseline 모델은 로지스틱 회귀입니다)\n","- 그 다음, 결과를 기록하여 Kaggle에 제출해봅시다!"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":43.097338,"end_time":"2023-05-08T06:21:49.406392","exception":false,"start_time":"2023-05-08T06:21:06.309054","status":"completed"},"tags":[]},"outputs":[],"source":["##########################################################################################\n","# Empty Module #7\n","# 지금까지 전처리한 데이터로 분류를 수행하여, kaggle에 제출해봅시다.\n","# Baseline은 로지스틱 회귀입니다.\n","##########################################################################################\n","# 분류기 정의 및 학습 수행 코드 작성\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# 여기에 코드 작성\n","logistic = LogisticRegression()\n","logistic.fit(x_train_BoW_reduced, y_train)\n","y_pred = logistic.predict(x_train_BoW_reduced)\n","print(\"정확도:\", accuracy_score(y_train, y_pred))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":275.661355,"end_time":"2023-05-08T06:26:28.190092","exception":false,"start_time":"2023-05-08T06:21:52.528737","status":"completed"},"tags":[]},"outputs":[],"source":["# TEST 데이터를 전처리\n","x_test = test_data[\"review\"]\n","x_test_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_test.iteritems(), total=len(x_test), desc=\"pre-processing data\")]\n","x_test_tokenized = [tokenize_words(x) for x in tqdm(x_test_preprocessed, desc=\"tokenizing data\")]\n","x_test_stopwords_excluded = [exclude_stopwords(x) for x in x_test_tokenized]\n","x_test_embedded = [embed_tokens(x, mode=\"TEST\") for x in tqdm(x_test_stopwords_excluded, desc=\"embedding data\")]\n","x_test_BoW = to_BoW_representation(x_test_embedded)\n","x_test_BoW_reduced = x_test_BoW[:, indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.030924,"end_time":"2023-05-08T06:26:29.528144","exception":false,"start_time":"2023-05-08T06:26:28.49722","status":"completed"},"tags":[]},"outputs":[],"source":["# TEST 데이터에 대한 예측 수행 코드 작성"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.489598,"end_time":"2023-05-08T06:26:30.391147","exception":false,"start_time":"2023-05-08T06:26:29.901549","status":"completed"},"tags":[]},"outputs":[],"source":["submit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\", index_col=0)\n","# TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.301876,"end_time":"2023-05-08T06:26:30.999594","exception":false,"start_time":"2023-05-08T06:26:30.697718","status":"completed"},"tags":[]},"source":["## \\[Empty Module #8\\] TF-IDF 적용\n","- BoW에서는 고려하지 않는 각 단어들의 중요도를 고려하기 위해, TF-IDF를 적용해봅시다.\n","- 캐글 프로젝트 설명 페이지의 설명을 참고하여 빈칸을 채워, TF-IDF를 구현해봅시다."]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.995899,"end_time":"2023-05-08T06:26:32.302887","exception":false,"start_time":"2023-05-08T06:26:31.306988","status":"completed"},"tags":[]},"outputs":[],"source":["##########################################################################################\n","# Empty Module #8\n","# 빈칸을 적절히 채워넣어 TF-IDF를 위한 Inverse Document Frequency를 계산해봅시다.\n","##########################################################################################\n","N = len(x_train_BoW)  # 총 데이터 샘플의 수\n","\n","def calculate_document_frequency(x):\n","    # 어떤 단어가 등장하는 데이터 샘플(문서)의 수(DF)를 계산하는 코드 작성\n","    df = np.where(x > 0, 1, 0)\n","    return df.sum(axis=0)\n","\n","\n","def calculate_inverse_document_frequency(document_frequency):\n","    # DF에 반비례하는 IDF를 계산하는 코드 작성\n","    return np.log(N / (calculate_document_frequency(x_train_BoW_reduced) + 1))\n","\n","document_frequency = calculate_document_frequency(x_train_BoW_reduced) # 여기에 코드 작성\n","inverse_document_frequency = calculate_inverse_document_frequency(x_train_BoW_reduced) # 여기에 코드 작성\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["array([ 4, 10, 18])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["a = np.array([1, 2, 3])\n","b = np.array([4, 5, 6])\n","a * b"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["array([0, 1, 1])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["np.where(a > 1, 1, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.56492,"end_time":"2023-05-08T06:26:34.1783","exception":false,"start_time":"2023-05-08T06:26:32.61338","status":"completed"},"tags":[]},"outputs":[],"source":["# 데이터에 위에서 구한 IDF를 곱하는 코드 작성\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.302103,"end_time":"2023-05-08T06:26:34.784772","exception":false,"start_time":"2023-05-08T06:26:34.482669","status":"completed"},"tags":[]},"source":["## \\[Empty Module #9\\]  분류 수행 및 제출: TF-IDF\n","- TF-IDF를 적용한 결과를 기록하여 Kaggle에 제출해봅시다!"]},{"cell_type":"code","execution_count":22,"metadata":{"papermill":{"duration":50.621027,"end_time":"2023-05-08T06:27:25.711221","exception":false,"start_time":"2023-05-08T06:26:35.090194","status":"completed"},"tags":[]},"outputs":[],"source":["##########################################################################################\n","# Empty Module #9\n","# TEST 데이터에 TF-IDF를 적용하여 모델을 학습, 예측을 수행하고 kaggle에 제출해봅시다.\n","# 이때, BoW와 같은 모델을 사용하여 성능을 비교해봅시다.\n","##########################################################################################\n","# 분류기 정의 및 학습 수행 코드 작성"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":1.234079,"end_time":"2023-05-08T06:27:28.588531","exception":false,"start_time":"2023-05-08T06:27:27.354452","status":"completed"},"tags":[]},"outputs":[],"source":["# TEST 데이터를 전처리하는 코드 작성"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.569811,"end_time":"2023-05-08T06:27:29.46472","exception":false,"start_time":"2023-05-08T06:27:28.894909","status":"completed"},"tags":[]},"outputs":[],"source":["# 예측을 수행하는 코드 작성"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.401319,"end_time":"2023-05-08T06:27:30.302375","exception":false,"start_time":"2023-05-08T06:27:29.901056","status":"completed"},"tags":[]},"outputs":[],"source":["submit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\", index_col=0)\n","# TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 결과 비교 TIP\n","- BoW와 TF-IDF에 같은 모델을 적용하여 성능의 차이를 비교해봅시다.\n","- 각각의 방법론에 여러가지 모델을 적용하며, 사용한 방법에 따라 더 적절한 모델이 있는지 고려해봅시다.\n","- 실험 결과들을 토대로, 왜 이런 결과가 도출되었는지 고민해봅시다."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
