{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install konlpy  # 토큰화에 사용할 konlpy 라이브러리 설치","metadata":{"papermill":{"duration":13.875729,"end_time":"2023-05-08T06:10:55.254188","exception":false,"start_time":"2023-05-08T06:10:41.378459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:21.417333Z","iopub.execute_input":"2023-05-27T02:00:21.417787Z","iopub.status.idle":"2023-05-27T02:00:37.809623Z","shell.execute_reply.started":"2023-05-27T02:00:21.417751Z","shell.execute_reply":"2023-05-27T02:00:37.808099Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting konlpy\n  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from konlpy) (4.9.2)\nCollecting JPype1>=0.7.0\n  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.10/site-packages (from konlpy) (1.23.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\nInstalling collected packages: JPype1, konlpy\nSuccessfully installed JPype1-1.4.1 konlpy-0.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, random\nfrom tqdm import tqdm # 진행도 시각화를 위한 라이브러리\nfrom sklearn.linear_model import LogisticRegression\n\nseed=42\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"papermill":{"duration":0.022219,"end_time":"2023-05-08T06:10:55.287013","exception":false,"start_time":"2023-05-08T06:10:55.264794","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:37.811659Z","iopub.execute_input":"2023-05-27T02:00:37.812036Z","iopub.status.idle":"2023-05-27T02:00:38.662142Z","shell.execute_reply.started":"2023-05-27T02:00:37.812003Z","shell.execute_reply":"2023-05-27T02:00:38.661140Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 데이터 불러오기","metadata":{"papermill":{"duration":0.009253,"end_time":"2023-05-08T06:10:55.306186","exception":false,"start_time":"2023-05-08T06:10:55.296933","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_train.csv\", index_col=0)\ntest_data = pd.read_csv(\"/kaggle/input/2023-ml-project1/nsmc_test.csv\", index_col=0)\nprint(train_data.shape)\nprint(test_data.shape)","metadata":{"papermill":{"duration":0.863577,"end_time":"2023-05-08T06:10:56.179319","exception":false,"start_time":"2023-05-08T06:10:55.315742","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:38.663739Z","iopub.execute_input":"2023-05-27T02:00:38.664190Z","iopub.status.idle":"2023-05-27T02:00:39.642269Z","shell.execute_reply.started":"2023-05-27T02:00:38.664148Z","shell.execute_reply":"2023-05-27T02:00:39.640686Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(149993, 2)\n(49999, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data.head()","metadata":{"papermill":{"duration":0.044922,"end_time":"2023-05-08T06:10:56.234099","exception":false,"start_time":"2023-05-08T06:10:56.189177","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:39.644832Z","iopub.execute_input":"2023-05-27T02:00:39.645487Z","iopub.status.idle":"2023-05-27T02:00:39.673803Z","shell.execute_reply.started":"2023-05-27T02:00:39.645453Z","shell.execute_reply":"2023-05-27T02:00:39.672562Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                  review  rating\nid                                              \n9324809       배우들의 인생연기가 돋보였던... 최고의 드라마       1\n9305425              아 혜리 보고싶다 ... 여군좀 ㅠ       0\n5239110  눈이 팅팅..... 정말 ,..... 대박이다......       1\n9148159                 캐슬린 터너의 보디는 볼만했다       0\n6144938                         진짜 최고였다.       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9324809</th>\n      <td>배우들의 인생연기가 돋보였던... 최고의 드라마</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9305425</th>\n      <td>아 혜리 보고싶다 ... 여군좀 ㅠ</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5239110</th>\n      <td>눈이 팅팅..... 정말 ,..... 대박이다......</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9148159</th>\n      <td>캐슬린 터너의 보디는 볼만했다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6144938</th>\n      <td>진짜 최고였다.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x_train = train_data[\"review\"]\ny_train = np.array(train_data[\"rating\"])","metadata":{"papermill":{"duration":0.032904,"end_time":"2023-05-08T06:10:56.282527","exception":false,"start_time":"2023-05-08T06:10:56.249623","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:39.675196Z","iopub.execute_input":"2023-05-27T02:00:39.675655Z","iopub.status.idle":"2023-05-27T02:00:39.682650Z","shell.execute_reply.started":"2023-05-27T02:00:39.675623Z","shell.execute_reply":"2023-05-27T02:00:39.680948Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 자연어 전처리\n## \\[Empty Module #1\\] 데이터 전처리\n### 데이터 전처리 수행\n> 먼저, 리뷰를 분류하는데 도움이 되거나, 머신러닝 처리에 어려운 단어들을 제거해봅시다.\n1. 아래 조건에 맞는 정규표현식을 작성하여 영어와 한글 문자를 제외한 특수문자나 이모지, 숫자 등을 제거해봅시다.\n  - <mark>한글 문자(초성 제외), 영어 대문자, 영어 소문자, 띄어쓰기 이외의 문자를 제외</mark>하는 정규표현식 작성\n2. 영어 단어의 경우 같은 단어들이 같은 토큰으로 분류될 수 있도록 <mark>대문자로 통일</mark>해줍니다.","metadata":{"papermill":{"duration":0.01003,"end_time":"2023-05-08T06:10:56.302834","exception":false,"start_time":"2023-05-08T06:10:56.292804","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import re\n\ndef apply_regex(pattern, text):  # 정규표현식을 이용한 필터링 적용\n    text = re.sub(pattern, \"\", text)  # 정규표현식 패턴에 맞는 값들을 텍스트에서 제거\n    text = text.upper()# 영어들을 찾아 대문자로 치환하는 코드 작성\n    return text\n\ntext = \"안녕 Hello!!!:)ㅎㅎ반갑다.ㅠㅠ__\"\n#pattern = '[^가-힣A-Za-z]'\npattern = '[^\\w\\s]|[ㄱ-ㅎ]|[ㅏ-ㅠ]|_'\noutput = apply_regex(pattern, text)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:00:39.684837Z","iopub.execute_input":"2023-05-27T02:00:39.685800Z","iopub.status.idle":"2023-05-27T02:00:39.698623Z","shell.execute_reply.started":"2023-05-27T02:00:39.685759Z","shell.execute_reply":"2023-05-27T02:00:39.697434Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"안녕 HELLO반갑다\n","output_type":"stream"}]},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #1\n# 입력: 자연어 상태의 리뷰 텍스트\n# 출력: 한글(초성 제외), 영어 대문자, 띄어쓰기로만 구성된 텍스트 \n# 입력 예시: \"안녕 Hello!!!:)ㅎㅎ반갑다.\"\n# 출력 예시: \"안녕 HELLO반갑다\"\n##########################################################################################\nimport re\n\n# 여기에 정규표현식 코드 작성\npattern = '[^\\w\\s]|[ㄱ-ㅎ]|[ㅏ-ㅣ]|_'\n\ndef apply_regex(pattern, text):  # 정규표현식을 이용한 필터링 적용\n    text = re.sub(pattern, \"\", text)  # 정규표현식 패턴에 맞는 값들을 텍스트에서 제거\n    text = text.upper() # 영어들을 찾아 대문자로 치환하는 코드 작성\n    return text\n\nx_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]","metadata":{"papermill":{"duration":0.982216,"end_time":"2023-05-08T06:10:57.296264","exception":false,"start_time":"2023-05-08T06:10:56.314048","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:39.700290Z","iopub.execute_input":"2023-05-27T02:00:39.700652Z","iopub.status.idle":"2023-05-27T02:00:41.073386Z","shell.execute_reply.started":"2023-05-27T02:00:39.700623Z","shell.execute_reply":"2023-05-27T02:00:41.072151Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1444518553.py:18: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  x_train_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_train.iteritems(), total=len(x_train), desc=\"pre-processing data\")]\npre-processing data: 100%|██████████| 149993/149993 [00:01<00:00, 110794.84it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train_preprocessed[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:00:41.075056Z","iopub.execute_input":"2023-05-27T02:00:41.075673Z","iopub.status.idle":"2023-05-27T02:00:41.084168Z","shell.execute_reply.started":"2023-05-27T02:00:41.075639Z","shell.execute_reply":"2023-05-27T02:00:41.082824Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'배우들의 인생연기가 돋보였던 최고의 드라마'"},"metadata":{}}]},{"cell_type":"markdown","source":"## \\[Empty Module #2\\] 단어 토큰화\n### Open Korean Text(OKT)를 이용한 단어 토큰화(Tokenization)\n- 한국어 자연어 처리 라이브러리인 konlpy의 OKT 래퍼를 통하여 문장을 단어로 토큰화해봅시다.\n- 아래 도큐먼트를 참고하여 토큰화를 수행합니다.\n  - <mark>이때, OKT 클래스의 특정 매개변수를 이용해 어근화를 진행해줍니다. (도큐먼트 참고)</mark>\n- konlpy 도큐먼트: https://konlpy.org/ko/latest/api/konlpy.tag/#okt-class\n- OKT 도큐먼트: https://github.com/open-korean-text/open-korean-text","metadata":{"papermill":{"duration":0.014265,"end_time":"2023-05-08T06:10:57.323679","exception":false,"start_time":"2023-05-08T06:10:57.309414","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #2\n# 입력: 자연어 상태의 리뷰 데이터\n# 출력: 토큰화와 과정을 거쳐 단어들의 리스트로 변환된 데이터\n# 입력 예시: \"커피는 역시 학생회관 커피\"\n# 출력 예시: [\"커피\", \"는\", \"역시\", \"학생\", \"회관\", \"커피\"]\n##########################################################################################\nfrom konlpy.tag import Okt\nokt = Okt()\n\ndef tokenize_words(sentence):\n    sentence_tokenized = okt.morphs(sentence) # 여기에 코드 작성\n    return sentence_tokenized","metadata":{"papermill":{"duration":1.373034,"end_time":"2023-05-08T06:10:58.70747","exception":false,"start_time":"2023-05-08T06:10:57.334436","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:41.085416Z","iopub.execute_input":"2023-05-27T02:00:41.086021Z","iopub.status.idle":"2023-05-27T02:00:42.621796Z","shell.execute_reply.started":"2023-05-27T02:00:41.085988Z","shell.execute_reply":"2023-05-27T02:00:42.620575Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 약 10-15분 정도 소요됩니다. \nx_train_tokenized = [tokenize_words(x) for x in tqdm(x_train_preprocessed, desc=\"tokenizing data\")]","metadata":{"papermill":{"duration":573.138395,"end_time":"2023-05-08T06:20:31.856663","exception":false,"start_time":"2023-05-08T06:10:58.718268","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:00:42.626259Z","iopub.execute_input":"2023-05-27T02:00:42.626659Z","iopub.status.idle":"2023-05-27T02:10:17.428241Z","shell.execute_reply.started":"2023-05-27T02:00:42.626625Z","shell.execute_reply":"2023-05-27T02:10:17.427047Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"tokenizing data: 100%|██████████| 149993/149993 [09:34<00:00, 260.95it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## \\[Empty Module #3\\] 불용어 제거\n- 조사를 비롯한 불용어들은 많은 횟수 등장하지만, 리뷰의 긍정과 부정 여부를 판단하는데는 도움이 되지 않습니다.\n- 데이터에서 아래 리스트로 정의된 불용어들을 제거해줍니다.","metadata":{"papermill":{"duration":0.226231,"end_time":"2023-05-08T06:20:32.313486","exception":false,"start_time":"2023-05-08T06:20:32.087255","status":"completed"},"tags":[]}},{"cell_type":"code","source":"stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']  #별다른 의미가 없는 불용어들\n\ndef exclude_stopwords(text):\n    text = [word for word in text if not word in stopwords] # 위 리스트에 포함된 불용어들을 제거하는 코드 작성\n    return text\n\nx_train_stopwords_excluded = [exclude_stopwords(x) for x in x_train_tokenized]","metadata":{"papermill":{"duration":1.425199,"end_time":"2023-05-08T06:20:33.964564","exception":false,"start_time":"2023-05-08T06:20:32.539365","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:10:17.430239Z","iopub.execute_input":"2023-05-27T02:10:17.431057Z","iopub.status.idle":"2023-05-27T02:10:18.692369Z","shell.execute_reply.started":"2023-05-27T02:10:17.431012Z","shell.execute_reply":"2023-05-27T02:10:18.691138Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"x_train_stopwords_excluded[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:10:18.694323Z","iopub.execute_input":"2023-05-27T02:10:18.694795Z","iopub.status.idle":"2023-05-27T02:10:18.702417Z","shell.execute_reply.started":"2023-05-27T02:10:18.694751Z","shell.execute_reply":"2023-05-27T02:10:18.701056Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['배우', '인생', '연기', '돋보였던', '최고', '드라마']"},"metadata":{}}]},{"cell_type":"markdown","source":"## \\[Empty Module #4\\] 단어 임베딩\n### 단어 임베딩 코드 구현\n- 토큰화를 거쳐 분리된 단어들을 하나의 정수 값으로 매핑해주는 희소 표현법을 직접 구현해봅시다.\n- 입력된 단어가 새로운 단어라면 새로운 정수 값을 할당하고, 이전에 등장한 단어라면 이전에 할당한 정수를 할당하는 함수를 작성합니다.\n  - 단, <mark>테스트 데이터에 대해서는 새로운 단어가 등장하면 값을 할당하지 않습니다.</mark>","metadata":{"papermill":{"duration":0.227527,"end_time":"2023-05-08T06:20:34.416544","exception":false,"start_time":"2023-05-08T06:20:34.189017","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #4\n# 입력: 단어 토큰화된 데이터\n# 출력: 임베딩 과정을 거쳐, 각 단어가 하나의 실수 값으로 표현된 데이터\n# 입력 예시: [\"커피\", \"역시\", \"학생\", \"회관\", \"커피\"]\n# 출력 예시: [0, 1, 2, 3, 0]\n##########################################################################################\n\nembedding_dict = dict()  # 단어 임베딩을 위한 딕셔너리\nembedding_value = 0\n\ndef embed_tokens(sentence_tokenized, mode):\n    assert mode.upper() in [\"TRAIN\", \"TEST\"]\n    global embedding_value\n    \n    sentence_embedded = list()\n    for word in sentence_tokenized:\n        # 코드 작성\n        if mode.upper() in \"TRAIN\":\n            if word not in embedding_dict.keys():\n                embedding_dict[word] = embedding_value\n                embedding_value += 1\n            sentence_embedded.append(embedding_dict[word])\n        elif mode.upper() in \"TEST\":\n            if word in embedding_dict.keys():\n                sentence_embedded.append(embedding_dict[word])\n            \n        \n    \n    return sentence_embedded","metadata":{"papermill":{"duration":0.236545,"end_time":"2023-05-08T06:20:34.876645","exception":false,"start_time":"2023-05-08T06:20:34.6401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:10:18.704411Z","iopub.execute_input":"2023-05-27T02:10:18.704989Z","iopub.status.idle":"2023-05-27T02:10:18.714479Z","shell.execute_reply.started":"2023-05-27T02:10:18.704958Z","shell.execute_reply":"2023-05-27T02:10:18.713533Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 실행 시간이 제법 소요됩니다. 비정상이 아니니 걱정하지 않으셔도 됩니다.\nx_train_embedded = [embed_tokens(x, mode=\"TRAIN\") for x in tqdm(x_train_stopwords_excluded, desc=\"embedding data\")]\nprint(x_train_embedded[:5])\nprint(\"총 %d개의 단어가 임베딩되었습니다.\"%(embedding_value))","metadata":{"papermill":{"duration":1.328669,"end_time":"2023-05-08T06:20:36.428533","exception":false,"start_time":"2023-05-08T06:20:35.099864","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:10:18.716240Z","iopub.execute_input":"2023-05-27T02:10:18.716972Z","iopub.status.idle":"2023-05-27T02:10:20.848749Z","shell.execute_reply.started":"2023-05-27T02:10:18.716930Z","shell.execute_reply":"2023-05-27T02:10:20.847523Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"embedding data: 100%|██████████| 149993/149993 [00:02<00:00, 70853.96it/s]","output_type":"stream"},{"name":"stdout","text":"[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19, 20, 21], [22, 4, 23]]\n총 103790개의 단어가 임베딩되었습니다.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## \\[Empty Module #5\\] 문장 벡터화\n### Bag of Words 방법을 사용한 문장 벡터화\n- 캐글 프로젝트 설명 페이지의 Bag of Words 방법 설명을 참고하여 Bag of Words 방법을 직접 구현해봅시다.","metadata":{"papermill":{"duration":0.284019,"end_time":"2023-05-08T06:20:36.936606","exception":false,"start_time":"2023-05-08T06:20:36.652587","status":"completed"},"tags":[]}},{"cell_type":"code","source":"x_train_embedded[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:10:20.850360Z","iopub.execute_input":"2023-05-27T02:10:20.850930Z","iopub.status.idle":"2023-05-27T02:10:20.859753Z","shell.execute_reply.started":"2023-05-27T02:10:20.850888Z","shell.execute_reply":"2023-05-27T02:10:20.858306Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[0, 1, 2, 3, 4, 5]"},"metadata":{}}]},{"cell_type":"code","source":"embedding_value","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:10:20.861237Z","iopub.execute_input":"2023-05-27T02:10:20.861844Z","iopub.status.idle":"2023-05-27T02:10:20.873167Z","shell.execute_reply.started":"2023-05-27T02:10:20.861804Z","shell.execute_reply":"2023-05-27T02:10:20.871936Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"103790"},"metadata":{}}]},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #5\n# 입력: 임베딩 과정을 거친 데이터\n# 출력: BoW 형태로 변환되어, M차원의 고정된 크기를 가진 벡터로 변환된 데이터\n# 힌트: np.zeros((2, 3))는 [2, 3] 크기의 0으로 가득 찬 행렬을 생성합니다.\n##########################################################################################\n\nM = embedding_value# 전체 단어의 수\n\ndef to_BoW_representation(x):\n    global M\n    shape = (len(x), M) # BoW는 어떤 shape를 가져야 할까요?\n    x_BoW = np.zeros(shape)\n    for i in tqdm(range(len(x)), desc=\"making BoW representation\"):\n        \n        # 여기에 BoW 구현\n        for j in range(len(x[i])):\n            x_BoW[i][x[i][j]] += 1\n\n    return x_BoW\nx_train_BoW = to_BoW_representation(x_train_embedded)","metadata":{"papermill":{"duration":7.419859,"end_time":"2023-05-08T06:20:44.579436","exception":false,"start_time":"2023-05-08T06:20:37.159577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:10:20.874634Z","iopub.execute_input":"2023-05-27T02:10:20.875160Z","iopub.status.idle":"2023-05-27T02:10:30.043874Z","shell.execute_reply.started":"2023-05-27T02:10:20.875117Z","shell.execute_reply":"2023-05-27T02:10:30.042673Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"making BoW representation: 100%|██████████| 149993/149993 [00:09<00:00, 16383.99it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## \\[Empty Module #6\\] 차원 축소\n- 우리가 만든 BoW는 수많은 리뷰에 등장하는 모든 단어들을 사용하여 만들어졌기 때문에, 엄청난 양의 단어들을 가지고 있습니다.\n- 그러나, 실제로 영화 리뷰에 쓰이는 단어들은 이보다 적기 때문에, 많은 단어들이 전체 데이터에서 실제로는 한번도 등장하지 않거나, 매우 조금 등장하면서 공간을 차지하고 있을 것 입니다.\n- 데이터의 크기를 줄여 머신러닝 모델이 중요한 정보에 집중할 수 있도록 해봅시다.\n- <mark>학습 데이터에서 50번 미만으로 등장한 단어들을 제외</mark>해줍니다.","metadata":{"papermill":{"duration":0.226259,"end_time":"2023-05-08T06:20:45.030446","exception":false,"start_time":"2023-05-08T06:20:44.804187","status":"completed"},"tags":[]}},{"cell_type":"code","source":"x_train_BoW","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:10:30.045222Z","iopub.execute_input":"2023-05-27T02:10:30.045638Z","iopub.status.idle":"2023-05-27T02:10:30.053439Z","shell.execute_reply.started":"2023-05-27T02:10:30.045609Z","shell.execute_reply":"2023-05-27T02:10:30.052193Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([[1., 1., 1., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #6\n# 입력: BoW 형태로 변환된 (N, M) 크기의 데이터\n# 출력: 등장 빈도가 적은 단어들을 제외한 (N, m) 크기의 더 작은 데이터\n##########################################################################################\n\ndef exclude_rare_words(x, limit=50):\n    # 코드 작성\n    x_sum = x.sum(axis=0)\n    indices = np.where(x_sum >= limit)[0]\n    x_BoW = x[:, indices]\n    return x_BoW, indices\n\nx_train_BoW_reduced, indices = exclude_rare_words(x_train_BoW, limit=50)","metadata":{"papermill":{"duration":0.236099,"end_time":"2023-05-08T06:20:45.493465","exception":false,"start_time":"2023-05-08T06:20:45.257366","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:10:30.055140Z","iopub.execute_input":"2023-05-27T02:10:30.055558Z","iopub.status.idle":"2023-05-27T02:11:58.027683Z","shell.execute_reply.started":"2023-05-27T02:10:30.055520Z","shell.execute_reply":"2023-05-27T02:11:58.026390Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# 힌트\n# 1. 먼저 전체 데이터에서 각 단어가 등장한 횟수를 세어보세요.\n# 2. 그 다음, 등장 횟수가 50회 미만인 단어들을 찾습니다.\n# 3. 해당 단어들을 데이터에서 제거하는 코드를 작성합니다.\n# 4. 설계를 잘 하고 구현을 시작해야 어렵지 않습니다.","metadata":{"papermill":{"duration":7.501002,"end_time":"2023-05-08T06:20:53.222732","exception":false,"start_time":"2023-05-08T06:20:45.72173","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:11:58.029324Z","iopub.execute_input":"2023-05-27T02:11:58.029683Z","iopub.status.idle":"2023-05-27T02:11:58.035078Z","shell.execute_reply.started":"2023-05-27T02:11:58.029654Z","shell.execute_reply":"2023-05-27T02:11:58.033954Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# 여기에 코드 작성\n\nprint(\"원본 BoW 크기:\", x_train_BoW.shape)\nprint(\"차원 축소 후 크기:\", x_train_BoW_reduced.shape)","metadata":{"papermill":{"duration":12.114323,"end_time":"2023-05-08T06:21:05.627314","exception":false,"start_time":"2023-05-08T06:20:53.512991","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:11:58.036506Z","iopub.execute_input":"2023-05-27T02:11:58.036848Z","iopub.status.idle":"2023-05-27T02:11:58.048144Z","shell.execute_reply.started":"2023-05-27T02:11:58.036820Z","shell.execute_reply":"2023-05-27T02:11:58.046965Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"원본 BoW 크기: (149993, 103790)\n차원 축소 후 크기: (149993, 3724)\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train_BoW_reduced","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:11:58.050007Z","iopub.execute_input":"2023-05-27T02:11:58.050496Z","iopub.status.idle":"2023-05-27T02:11:58.063218Z","shell.execute_reply.started":"2023-05-27T02:11:58.050456Z","shell.execute_reply":"2023-05-27T02:11:58.061891Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([[1., 1., 1., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"x_train_BoW_reduced.sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:11:58.064384Z","iopub.execute_input":"2023-05-27T02:11:58.064726Z","iopub.status.idle":"2023-05-27T02:11:58.747790Z","shell.execute_reply.started":"2023-05-27T02:11:58.064700Z","shell.execute_reply":"2023-05-27T02:11:58.746653Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([ 5.,  2.,  4., ...,  6., 15.,  1.])"},"metadata":{}}]},{"cell_type":"markdown","source":"## \\[Empty Module #7\\]  분류 수행 및 제출: Bag of Words\n- 이제 모든 문장이 고정된 크기 $m$ 차원의 벡터로 변환되었습니다.\n- 원하는 모델을 사용하여, 각 문장의 영화에 대한 긍정적인 리뷰인지, 부정적인 리뷰인지 분류해봅시다.(Baseline 모델은 로지스틱 회귀입니다)\n- 그 다음, 결과를 기록하여 Kaggle에 제출해봅시다!","metadata":{"papermill":{"duration":0.22558,"end_time":"2023-05-08T06:21:06.079455","exception":false,"start_time":"2023-05-08T06:21:05.853875","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #7\n# 지금까지 전처리한 데이터로 분류를 수행하여, kaggle에 제출해봅시다.\n# Baseline은 로지스틱 회귀입니다.\n##########################################################################################\n# 분류기 정의 및 학습 수행 코드 작성\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(random_state=seed)","metadata":{"papermill":{"duration":43.097338,"end_time":"2023-05-08T06:21:49.406392","exception":false,"start_time":"2023-05-08T06:21:06.309054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:11:58.749463Z","iopub.execute_input":"2023-05-27T02:11:58.750561Z","iopub.status.idle":"2023-05-27T02:11:58.756774Z","shell.execute_reply.started":"2023-05-27T02:11:58.750522Z","shell.execute_reply":"2023-05-27T02:11:58.755161Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train_BoW_reduced, y_train)\n#print(model.score(x_train_BoW_reduced))","metadata":{"execution":{"iopub.status.busy":"2023-05-27T02:11:58.759381Z","iopub.execute_input":"2023-05-27T02:11:58.760412Z","iopub.status.idle":"2023-05-27T02:12:59.507541Z","shell.execute_reply.started":"2023-05-27T02:11:58.760373Z","shell.execute_reply":"2023-05-27T02:12:59.505970Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"LogisticRegression(random_state=42)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# TEST 데이터를 전처리 (3분 정도걸림 주의)\nx_test = test_data[\"review\"]\nx_test_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_test.iteritems(), total=len(x_test), desc=\"pre-processing data\")]\nx_test_tokenized = [tokenize_words(x) for x in tqdm(x_test_preprocessed, desc=\"tokenizing data\")]\nx_test_stopwords_excluded = [exclude_stopwords(x) for x in x_test_tokenized]\nx_test_embedded = [embed_tokens(x, mode=\"TEST\") for x in tqdm(x_test_stopwords_excluded, desc=\"embedding data\")]\nx_test_BoW = to_BoW_representation(x_test_embedded)\nx_test_BoW_reduced = x_test_BoW[:, indices]","metadata":{"papermill":{"duration":275.661355,"end_time":"2023-05-08T06:26:28.190092","exception":false,"start_time":"2023-05-08T06:21:52.528737","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:12:59.515596Z","iopub.execute_input":"2023-05-27T02:12:59.516843Z","iopub.status.idle":"2023-05-27T02:17:23.719115Z","shell.execute_reply.started":"2023-05-27T02:12:59.516777Z","shell.execute_reply":"2023-05-27T02:17:23.717918Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1083764141.py:3: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  x_test_preprocessed = [apply_regex(pattern, str(x[1])) for x in tqdm(x_test.iteritems(), total=len(x_test), desc=\"pre-processing data\")]\npre-processing data: 100%|██████████| 49999/49999 [00:00<00:00, 101962.97it/s]\ntokenizing data: 100%|██████████| 49999/49999 [04:06<00:00, 202.61it/s]\nembedding data: 100%|██████████| 49999/49999 [00:00<00:00, 79737.15it/s]\nmaking BoW representation: 100%|██████████| 49999/49999 [00:02<00:00, 18498.34it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# TEST 데이터에 대한 예측 수행 코드 작성\ny_test_pred = model.predict(x_test_BoW_reduced)","metadata":{"papermill":{"duration":1.030924,"end_time":"2023-05-08T06:26:29.528144","exception":false,"start_time":"2023-05-08T06:26:28.49722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-27T02:17:23.720375Z","iopub.execute_input":"2023-05-27T02:17:23.720704Z","iopub.status.idle":"2023-05-27T02:17:23.947368Z","shell.execute_reply.started":"2023-05-27T02:17:23.720677Z","shell.execute_reply":"2023-05-27T02:17:23.945703Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\", index_col=0)\n# TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성\nsubmit['rating'] = y_test_pred\nsubmit.to_csv(\"submit.csv\", header=True, mode='w')\nsubmit\n","metadata":{"papermill":{"duration":0.489598,"end_time":"2023-05-08T06:26:30.391147","exception":false,"start_time":"2023-05-08T06:26:29.901549","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-25T13:43:47.202586Z","iopub.execute_input":"2023-05-25T13:43:47.204117Z","iopub.status.idle":"2023-05-25T13:43:47.444677Z","shell.execute_reply.started":"2023-05-25T13:43:47.204039Z","shell.execute_reply":"2023-05-25T13:43:47.443417Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"         rating\nid             \n9503843       1\n3676359       1\n6736987       0\n6916831       1\n9458520       1\n...         ...\n8771102       0\n6947729       1\n9539166       0\n8480745       0\n9977293       0\n\n[49999 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9503843</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3676359</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6736987</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6916831</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9458520</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8771102</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6947729</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9539166</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8480745</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9977293</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>49999 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## \\[Empty Module #8\\] TF-IDF 적용\n- BoW에서는 고려하지 않는 각 단어들의 중요도를 고려하기 위해, TF-IDF를 적용해봅시다.\n- 캐글 프로젝트 설명 페이지의 설명을 참고하여 빈칸을 채워, TF-IDF를 구현해봅시다.","metadata":{"papermill":{"duration":0.301876,"end_time":"2023-05-08T06:26:30.999594","exception":false,"start_time":"2023-05-08T06:26:30.697718","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #8\n# 빈칸을 적절히 채워넣어 TF-IDF를 위한 Inverse Document Frequency를 계산해봅시다.\n##########################################################################################\nN = len(x_train_BoW)  # 총 데이터 샘플의 수\n\n\ndef calculate_document_frequency(x):\n    # 어떤 단어가 등장하는 데이터 샘플(문서)의 수(DF)를 계산하는 코드 작성\n    document_frequency = np.where(x > 0, 1, 0)\n    return document_frequency.sum(axis=0)\n\ndef calculate_inverse_document_frequency(document_frequency):\n    # DF에 반비례하는 IDF를 계산하는 코드 작성\n    global N\n    return np.log(N / (document_frequency + 1))\n\ndocument_frequency = calculate_document_frequency(x_train_BoW) # 여기에 코드 작성\ninverse_document_frequency = calculate_inverse_document_frequency(document_frequency) # 여기에 코드 작성","metadata":{"papermill":{"duration":0.995899,"end_time":"2023-05-08T06:26:32.302887","exception":false,"start_time":"2023-05-08T06:26:31.306988","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-25T13:56:12.394202Z","iopub.execute_input":"2023-05-25T13:56:12.394673Z","iopub.status.idle":"2023-05-25T13:56:16.922773Z","shell.execute_reply.started":"2023-05-25T13:56:12.394639Z","shell.execute_reply":"2023-05-25T13:56:16.921027Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(type(inverse_document_frequency), inverse_document_frequency.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T13:07:57.773555Z","iopub.execute_input":"2023-05-25T13:07:57.774503Z","iopub.status.idle":"2023-05-25T13:07:57.780371Z","shell.execute_reply.started":"2023-05-25T13:07:57.774472Z","shell.execute_reply":"2023-05-25T13:07:57.779102Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'> (103790,)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(x_train_BoW), x_train_BoW.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T13:07:57.782688Z","iopub.execute_input":"2023-05-25T13:07:57.784246Z","iopub.status.idle":"2023-05-25T13:07:57.794121Z","shell.execute_reply.started":"2023-05-25T13:07:57.784206Z","shell.execute_reply":"2023-05-25T13:07:57.792695Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'> (149993, 103790)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 데이터에 위에서 구한 IDF를 곱하는 코드 작성\nx_train_tfidf = np.array([x * inverse_document_frequency for x in x_train_BoW])","metadata":{"papermill":{"duration":1.56492,"end_time":"2023-05-08T06:26:34.1783","exception":false,"start_time":"2023-05-08T06:26:32.61338","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-25T13:08:17.493001Z","iopub.execute_input":"2023-05-25T13:08:17.493425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## \\[Empty Module #9\\]  분류 수행 및 제출: TF-IDF\n- TF-IDF를 적용한 결과를 기록하여 Kaggle에 제출해봅시다!","metadata":{"papermill":{"duration":0.302103,"end_time":"2023-05-08T06:26:34.784772","exception":false,"start_time":"2023-05-08T06:26:34.482669","status":"completed"},"tags":[]}},{"cell_type":"code","source":"##########################################################################################\n# Empty Module #9\n# TEST 데이터에 TF-IDF를 적용하여 모델을 학습, 예측을 수행하고 kaggle에 제출해봅시다.\n# 이때, BoW와 같은 모델을 사용하여 성능을 비교해봅시다.\n##########################################################################################\n\n# 분류기 정의 및 학습 수행 코드 작성\nmodel_tfidf = LogisticRegression(random_state=seed)\nmodel_tfidf.fit(x_train_tfidf, y_train)\n# print(model_tfidf.score(x_train_tfidf))","metadata":{"papermill":{"duration":50.621027,"end_time":"2023-05-08T06:27:25.711221","exception":false,"start_time":"2023-05-08T06:26:35.090194","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST 데이터를 전처리하는 코드 작성\nN = len(x_test_BoW)\n\ndocument_frequency = calculate_document_frequency(x_test_BoW)\ninverse_document_frequency = calculate_inverse_document_frequency(document_frequency)\n\nx_test_tfidf = np.array([x * inverse_document_frequency for x in x_test_BoW])","metadata":{"papermill":{"duration":1.234079,"end_time":"2023-05-08T06:27:28.588531","exception":false,"start_time":"2023-05-08T06:27:27.354452","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 예측을 수행하는 코드 작성\ny_test_tfidf_pred = model_tfidf.predict(x_test_tfidf)","metadata":{"papermill":{"duration":0.569811,"end_time":"2023-05-08T06:27:29.46472","exception":false,"start_time":"2023-05-08T06:27:28.894909","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_2 = pd.read_csv(\"/kaggle/input/2023-ml-project1/sample_submission.csv\", index_col=0)\n# TEST 데이터에 대한 예측 값을 csv로 저장하는 코드 작성\nsubmit_2['rating'] = y_test_tfidf_pred\nsubmit_2.to_csv(\"submit_2.csv\", header=True, mode='w')\nsubmit_2","metadata":{"papermill":{"duration":0.401319,"end_time":"2023-05-08T06:27:30.302375","exception":false,"start_time":"2023-05-08T06:27:29.901056","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 결과 비교 TIP\n- BoW와 TF-IDF에 같은 모델을 적용하여 성능의 차이를 비교해봅시다.\n- 각각의 방법론에 여러가지 모델을 적용하며, 사용한 방법에 따라 더 적절한 모델이 있는지 고려해봅시다.\n- 실험 결과들을 토대로, 왜 이런 결과가 도출되었는지 고민해봅시다.","metadata":{}}]}